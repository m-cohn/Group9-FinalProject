{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_onset</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>filename</th>\n",
       "      <th>native_language</th>\n",
       "      <th>sex</th>\n",
       "      <th>speakerid</th>\n",
       "      <th>country</th>\n",
       "      <th>file_missing?</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>koussi, senegal</td>\n",
       "      <td>balanta</td>\n",
       "      <td>balanta</td>\n",
       "      <td>male</td>\n",
       "      <td>788</td>\n",
       "      <td>senegal</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>buea, cameroon</td>\n",
       "      <td>cameroon</td>\n",
       "      <td>cameroon</td>\n",
       "      <td>male</td>\n",
       "      <td>1953</td>\n",
       "      <td>cameroon</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>hong, adamawa, nigeria</td>\n",
       "      <td>fulfulde</td>\n",
       "      <td>fulfulde</td>\n",
       "      <td>male</td>\n",
       "      <td>1037</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>port-au-prince, haiti</td>\n",
       "      <td>haitian</td>\n",
       "      <td>haitian</td>\n",
       "      <td>male</td>\n",
       "      <td>1165</td>\n",
       "      <td>haiti</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>port-au-prince, haiti</td>\n",
       "      <td>haitian</td>\n",
       "      <td>haitian</td>\n",
       "      <td>male</td>\n",
       "      <td>1166</td>\n",
       "      <td>haiti</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  age_onset              birthplace  filename native_language   sex   \n",
       "0  24.0       12.0         koussi, senegal   balanta         balanta  male  \\\n",
       "1  18.0       10.0          buea, cameroon  cameroon        cameroon  male   \n",
       "2  48.0        8.0  hong, adamawa, nigeria  fulfulde        fulfulde  male   \n",
       "3  42.0       42.0   port-au-prince, haiti   haitian         haitian  male   \n",
       "4  40.0       35.0   port-au-prince, haiti   haitian         haitian  male   \n",
       "\n",
       "   speakerid   country  file_missing?  Unnamed: 9  Unnamed: 10 Unnamed: 11  \n",
       "0        788   senegal           True         NaN          NaN         NaN  \n",
       "1       1953  cameroon           True         NaN          NaN         NaN  \n",
       "2       1037   nigeria           True         NaN          NaN         NaN  \n",
       "3       1165     haiti           True         NaN          NaN         NaN  \n",
       "4       1166     haiti           True         NaN          NaN         NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert csv to dataframe\n",
    "df = pd.read_csv('speakers_all.csv')\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native_language\n",
      "english     579\n",
      "spanish     162\n",
      "arabic      102\n",
      "mandarin     65\n",
      "french       63\n",
      "           ... \n",
      "kalanga       1\n",
      "kabyle        1\n",
      "jola          1\n",
      "irish         1\n",
      "zulu          1\n",
      "Name: count, Length: 214, dtype: int64\n",
      "country\n",
      "usa         393\n",
      "china        88\n",
      "uk           67\n",
      "india        59\n",
      "canada       54\n",
      "           ... \n",
      "namibia       1\n",
      "romanian      1\n",
      "burundi       1\n",
      "rwanda        1\n",
      "benin         1\n",
      "Name: count, Length: 176, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print distribution of native_language, and country\n",
    "print(df['native_language'].value_counts())\n",
    "print(df['country'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english:  579\n",
      "Number of non-english:  1559\n",
      "Total number of files:  2172\n",
      "Number of english:  579\n",
      "Number of non-english:  780\n"
     ]
    }
   ],
   "source": [
    "# Create two classes of data, USA and non-USA, and grab the filenames of both classes\n",
    "# Also make sure the file_missing? column is False\n",
    "english = df[(df['native_language'] == 'english') & (df['file_missing?'] == False)]['filename']\n",
    "non_english = df[(df['native_language'] != 'english') & (df['file_missing?'] == False)]['filename']\n",
    "\n",
    "print('Number of english: ', len(english))\n",
    "print('Number of non-english: ', len(non_english))\n",
    "\n",
    "# Print total number of files\n",
    "print('Total number of files: ', len(df))\n",
    "\n",
    "# Create a list of countries that have less than 10 speakers\n",
    "native_languages = df['native_language'].value_counts()\n",
    "native_languages = native_languages[native_languages < 25].index.tolist()\n",
    "\n",
    "# Remove the countries that only have 1 speaker\n",
    "df = df[~df['native_language'].isin(native_languages)]\n",
    "\n",
    "english = df[(df['native_language'] == 'english') & (df['file_missing?'] == False)]['filename']\n",
    "non_english = df[(df['native_language'] != 'english') & (df['file_missing?'] == False)]['filename']\n",
    "\n",
    "print('Number of english: ', len(english))\n",
    "print('Number of non-english: ', len(non_english))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate the spectrogram\n",
    "def calculate_spectrogram(audio_file, n_fft=512, hop_length=256):\n",
    "    signal, sample_rate = librosa.load(audio_file)\n",
    "    spectrogram = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    return np.abs(spectrogram)\n",
    "\n",
    "def truncate_spectrogram(spectrogram, fixed_length):\n",
    "    truncated_spectrogram = spectrogram[:, :fixed_length]\n",
    "    return truncated_spectrogram\n",
    "\n",
    "def calculate_mel_spectrogram(audio_file, n_fft=512, hop_length=256, n_mels=128):\n",
    "    signal, sample_rate = librosa.load(audio_file)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    \n",
    "    return mel_spectrogram\n",
    "\n",
    "def calculate_mel_spectrogram_no_load(signal, sample_rate, n_fft=512, hop_length=256, n_mels=128):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    \n",
    "    return mel_spectrogram\n",
    "\n",
    "def calculate_log_mel_spectrogram(audio_file, n_fft=512, hop_length=256, n_mels=128):\n",
    "    signal, sample_rate = librosa.load(audio_file)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    \n",
    "    return log_mel_spectrogram\n",
    "\n",
    "def calculate_log_mel_spectrogram_no_load(signal, sample_rate, n_fft=512, hop_length=256, n_mels=128):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    \n",
    "    return log_mel_spectrogram\n",
    "\n",
    "def normalize_spectrogram(spectrogram):\n",
    "    mean = np.mean(spectrogram)\n",
    "    std_dev = np.std(spectrogram)\n",
    "    normalized_spectrogram = (spectrogram - mean) / std_dev\n",
    "    return normalized_spectrogram\n",
    "\n",
    "def min_max_normalize_spectrogram(spectrogram):\n",
    "    min_value = np.min(spectrogram)\n",
    "    max_value = np.max(spectrogram)\n",
    "    normalized_spectrogram = (spectrogram - min_value) / (max_value - min_value)\n",
    "    return normalized_spectrogram\n",
    "\n",
    "def pitch_shift(audio_data, sample_rate, pitch_shift_steps):\n",
    "    return librosa.effects.pitch_shift(y=audio_data, sr=sample_rate, n_steps=pitch_shift_steps)\n",
    "\n",
    "def add_background_noise(audio_data, noise_amplitude):\n",
    "    noise = np.random.normal(0, noise_amplitude, len(audio_data))\n",
    "    return audio_data + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613     english324\n",
       "886     english570\n",
       "923      english82\n",
       "828     english518\n",
       "718     english419\n",
       "           ...    \n",
       "1106       german9\n",
       "1877     spanish36\n",
       "1221     italian24\n",
       "1090      german27\n",
       "1315      korean23\n",
       "Name: filename, Length: 200, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take n samples from each class and use them to create a validation set\n",
    "english_validation = english.sample(n=125, random_state=42)\n",
    "non_english_validation = non_english.sample(n=125, random_state=42)\n",
    "\n",
    "# Remove the validation samples from the training set\n",
    "english = english.drop(english_validation.index)\n",
    "non_english = non_english.drop(non_english_validation.index)\n",
    "\n",
    "val_set = pd.concat([english_validation, non_english_validation])\n",
    "train_set = pd.concat([english, non_english])\n",
    "\n",
    "# Print 50 rows of the validation set\n",
    "val_set.head(200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Calculate the spectrogram for all audio files in the english class\n",
    "train_set_spectro = []\n",
    "train_set_labels = []\n",
    "i = 0\n",
    "for file in train_set:\n",
    "    # Check if file_name exists, if it doesn't, skip it\n",
    "    file_name = 'recordings/recordings/' + file + '.mp3'\n",
    "    if not os.path.exists(file_name):\n",
    "        continue\n",
    "\n",
    "    spectrogram = calculate_log_mel_spectrogram('recordings/recordings/' + file + '.mp3', n_fft=2048, hop_length=512)\n",
    "    train_set_spectro.append(spectrogram)\n",
    "\n",
    "    # Create a pitch shifted version of the spectrogram, first find the sampling rate\n",
    "    signal, sample_rate = librosa.load('recordings/recordings/' + file + '.mp3')\n",
    "    pitch_shifted_spectrogram = calculate_log_mel_spectrogram_no_load(signal=pitch_shift(signal, sample_rate, 2), sample_rate=sample_rate, n_fft=2048, hop_length=512)\n",
    "    train_set_spectro.append(pitch_shifted_spectrogram)\n",
    "\n",
    "    # Create another pitch shifted version of the spectrogram\n",
    "    pitch_shifted_spectrogram = calculate_log_mel_spectrogram_no_load(signal=pitch_shift(signal, sample_rate, -2), sample_rate=sample_rate, n_fft=2048, hop_length=512)\n",
    "    train_set_spectro.append(pitch_shifted_spectrogram)\n",
    "\n",
    "    # Create a noisy version of the spectrogram\n",
    "    noisy_spectrogram = calculate_log_mel_spectrogram_no_load(signal=add_background_noise(signal, 0.005), sample_rate=sample_rate, n_fft=2048, hop_length=512)\n",
    "    train_set_spectro.append(noisy_spectrogram)\n",
    "\n",
    "    # Create a noisy pitch shifted version of the spectrogram\n",
    "    noisy_pitch_shifted_spectrogram = calculate_log_mel_spectrogram_no_load(signal=add_background_noise(pitch_shift(signal, sample_rate, 2), 0.005), sample_rate=sample_rate, n_fft=2048, hop_length=512)\n",
    "    train_set_spectro.append(noisy_pitch_shifted_spectrogram)\n",
    "\n",
    "    # Create another noisy pitch shifted version of the spectrogram\n",
    "    noisy_pitch_shifted_spectrogram = calculate_log_mel_spectrogram_no_load(signal=add_background_noise(pitch_shift(signal, sample_rate, -2), 0.005), sample_rate=sample_rate, n_fft=2048, hop_length=512)\n",
    "    train_set_spectro.append(noisy_pitch_shifted_spectrogram)\n",
    "\n",
    "    # If the file name has english in it, it's an english speaker, otherwise it's a non-english speaker\n",
    "    if 'english' in file:\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "    else:\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "\n",
    "    # Add a counter to keep track of progress\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "# Grab the spectros for all audio files in the non-english class\n",
    "val_set_spectro = []\n",
    "val_set_labels = []\n",
    "i = 0\n",
    "for file in val_set:\n",
    "    # Check if file_name exists, if it doesn't, skip it\n",
    "    file_name = 'recordings/recordings/' + file + '.mp3'\n",
    "    if not os.path.exists(file_name):\n",
    "        continue\n",
    "\n",
    "    spectrogram = calculate_log_mel_spectrogram('recordings/recordings/' + file + '.mp3', n_fft=2048, hop_length=512)\n",
    "\n",
    "    val_set_spectro.append(spectrogram)\n",
    "\n",
    "    # No need to apply data augmentation to the validation set, add correct label\n",
    "    if 'english' in file:\n",
    "        val_set_labels.append(1)\n",
    "    else:\n",
    "        val_set_labels.append(0)\n",
    "\n",
    "    # Add a counter to keep track of progress\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "# Calculate the minimum length of all spectrograms\n",
    "min_length = min([spectrogram.shape[1] for spectrogram in train_set_spectro + val_set_spectro])\n",
    "\n",
    "# Truncate each spectrogram in the english_spectro and non_english_spectro lists to the minimum length\n",
    "train_set_spectro = [truncate_spectrogram(spectrogram, min_length) for spectrogram in train_set_spectro]\n",
    "val_set_spectro = [truncate_spectrogram(spectrogram, min_length) for spectrogram in val_set_spectro]\n",
    "\n",
    "# Normalize each spectrogram in the english_spectro and non_english_spectro lists using min-max normalization\n",
    "train_set_spectro = [min_max_normalize_spectrogram(spectrogram) for spectrogram in train_set_spectro]\n",
    "val_set_spectro = [min_max_normalize_spectrogram(spectrogram) for spectrogram in val_set_spectro]\n",
    "\n",
    "# Add channel dimension as the first dimension to each spectrogram\n",
    "train_set_spectro = [np.expand_dims(spectrogram, axis=0) for spectrogram in train_set_spectro]\n",
    "val_set_spectro = [np.expand_dims(spectrogram, axis=0) for spectrogram in val_set_spectro]\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "train_set_spectro = np.array(train_set_spectro)\n",
    "val_set_spectro = np.array(val_set_spectro)\n",
    "\n",
    "# Convert the labels to numpy arrays\n",
    "train_set_labels = np.array(train_set_labels)\n",
    "val_set_labels = np.array(val_set_labels)\n",
    "\n",
    "# Print shapes of first and last spectrogram in both lists\n",
    "print(train_set_spectro[0].shape)\n",
    "print(train_set_spectro[-1].shape)\n",
    "\n",
    "print(val_set_spectro[0].shape)\n",
    "print(val_set_spectro[-1].shape)\n",
    "\n",
    "# Print the shape of the arrays\n",
    "print(\"Size of training set: \", train_set_spectro.shape)\n",
    "print(\"Size of validation set: \", val_set_spectro.shape)\n",
    "\n",
    "# Count the number of english and non-english speakers in the training and validation sets\n",
    "print(\"Number of english speakers in training set: \", np.sum(train_set_labels))\n",
    "print(\"Number of non-english speakers in training set: \", len(train_set_labels) - np.sum(train_set_labels))\n",
    "\n",
    "print(\"Number of english speakers in validation set: \", np.sum(val_set_labels))\n",
    "print(\"Number of non-english speakers in validation set: \", len(val_set_labels) - np.sum(val_set_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a label of 1 to the english class, and 0 to the non-english class\n",
    "# english_labels = np.ones(english_spectro.shape[0])\n",
    "# non_english_labels = np.zeros(non_english_spectro.shape[0])\n",
    "# augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# # Combine the english and non-english data into one array\n",
    "# X = np.concatenate((english_spectro, non_english_spectro))\n",
    "# y = np.concatenate((english_labels, non_english_labels))\n",
    "\n",
    "# # Print the shape of the combined array\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to npz files\n",
    "# np.savez('mel_spectro_data_min_max_norm_augmented_v2.npz', X=X, y=y, X_augmented=augmented_spectro, y_augmented=augmented_labels)\n",
    "\n",
    "np.savez('log_mel_spectro_data_min_max_norm_augmented_v2.npz', X_train=train_set_spectro, y_train=train_set_labels, X_val=val_set_spectro, y_val=val_set_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
