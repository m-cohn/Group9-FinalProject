{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "from AudioAugmentations import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_onset</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>filename</th>\n",
       "      <th>native_language</th>\n",
       "      <th>sex</th>\n",
       "      <th>speakerid</th>\n",
       "      <th>country</th>\n",
       "      <th>file_missing?</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>koussi, senegal</td>\n",
       "      <td>balanta</td>\n",
       "      <td>balanta</td>\n",
       "      <td>male</td>\n",
       "      <td>788</td>\n",
       "      <td>senegal</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>buea, cameroon</td>\n",
       "      <td>cameroon</td>\n",
       "      <td>cameroon</td>\n",
       "      <td>male</td>\n",
       "      <td>1953</td>\n",
       "      <td>cameroon</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>hong, adamawa, nigeria</td>\n",
       "      <td>fulfulde</td>\n",
       "      <td>fulfulde</td>\n",
       "      <td>male</td>\n",
       "      <td>1037</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>port-au-prince, haiti</td>\n",
       "      <td>haitian</td>\n",
       "      <td>haitian</td>\n",
       "      <td>male</td>\n",
       "      <td>1165</td>\n",
       "      <td>haiti</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>port-au-prince, haiti</td>\n",
       "      <td>haitian</td>\n",
       "      <td>haitian</td>\n",
       "      <td>male</td>\n",
       "      <td>1166</td>\n",
       "      <td>haiti</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  age_onset              birthplace  filename native_language   sex   \n",
       "0  24.0       12.0         koussi, senegal   balanta         balanta  male  \\\n",
       "1  18.0       10.0          buea, cameroon  cameroon        cameroon  male   \n",
       "2  48.0        8.0  hong, adamawa, nigeria  fulfulde        fulfulde  male   \n",
       "3  42.0       42.0   port-au-prince, haiti   haitian         haitian  male   \n",
       "4  40.0       35.0   port-au-prince, haiti   haitian         haitian  male   \n",
       "\n",
       "   speakerid   country  file_missing?  Unnamed: 9  Unnamed: 10 Unnamed: 11  \n",
       "0        788   senegal           True         NaN          NaN         NaN  \n",
       "1       1953  cameroon           True         NaN          NaN         NaN  \n",
       "2       1037   nigeria           True         NaN          NaN         NaN  \n",
       "3       1165     haiti           True         NaN          NaN         NaN  \n",
       "4       1166     haiti           True         NaN          NaN         NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert csv to dataframe\n",
    "df = pd.read_csv('speakers_all.csv')\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native_language\n",
      "english     579\n",
      "spanish     162\n",
      "arabic      102\n",
      "mandarin     65\n",
      "french       63\n",
      "           ... \n",
      "kalanga       1\n",
      "kabyle        1\n",
      "jola          1\n",
      "irish         1\n",
      "zulu          1\n",
      "Name: count, Length: 214, dtype: int64\n",
      "country\n",
      "usa         393\n",
      "china        88\n",
      "uk           67\n",
      "india        59\n",
      "canada       54\n",
      "           ... \n",
      "namibia       1\n",
      "romanian      1\n",
      "burundi       1\n",
      "rwanda        1\n",
      "benin         1\n",
      "Name: count, Length: 176, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print distribution of native_language, and country\n",
    "print(df['native_language'].value_counts())\n",
    "print(df['country'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english:  579\n",
      "Number of non-english:  1559\n",
      "Total number of files:  2172\n",
      "Number of english:  579\n",
      "Number of non-english:  579\n"
     ]
    }
   ],
   "source": [
    "# Create two classes of data, USA and non-USA, and grab the filenames of both classes\n",
    "# Also make sure the file_missing? column is False\n",
    "english = df[(df['native_language'] == 'english') & (df['file_missing?'] == False)]['filename']\n",
    "non_english = df[(df['native_language'] != 'english') & (df['file_missing?'] == False)]['filename']\n",
    "\n",
    "print('Number of english: ', len(english))\n",
    "print('Number of non-english: ', len(non_english))\n",
    "\n",
    "# Print total number of files\n",
    "print('Total number of files: ', len(df))\n",
    "\n",
    "# Create a list of countries that have less than 10 speakers\n",
    "native_languages = df['native_language'].value_counts()\n",
    "native_languages = native_languages[native_languages < 20].index.tolist()\n",
    "\n",
    "# Remove the countries that only have 1 speaker\n",
    "df = df[~df['native_language'].isin(native_languages)]\n",
    "\n",
    "english = df[(df['native_language'] == 'english') & (df['file_missing?'] == False)]['filename']\n",
    "non_english = df[(df['native_language'] != 'english') & (df['file_missing?'] == False)]['filename']\n",
    "\n",
    "# Randomly remove files from non_english until the number of files is equal to the number of english files\n",
    "non_english = non_english.sample(n=len(english), random_state=42)\n",
    "\n",
    "print('Number of english: ', len(english))\n",
    "print('Number of non-english: ', len(non_english))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function for extracting the MFCCs from the audio files\n",
    "def extract_mfcc_1d(audio_file, n_mfcc=40, sample_rate=16000, load=True):\n",
    "    if load:\n",
    "        signal, sample_rate = librosa.load(audio_file)\n",
    "        mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=n_mfcc, n_fft=512, hop_length=256)\n",
    "        return np.mean(mfcc, axis=1)\n",
    "    \n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=audio_file, sr=sample_rate, n_mfcc=n_mfcc, n_fft=512, hop_length=256)\n",
    "    return np.mean(mfcc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613       english324\n",
       "886       english570\n",
       "923        english82\n",
       "828       english518\n",
       "718       english419\n",
       "            ...     \n",
       "1681       romanian4\n",
       "1640    portuguese42\n",
       "1700       russian19\n",
       "1314        korean22\n",
       "2047       turkish13\n",
       "Name: filename, Length: 200, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take n samples from each class and use them to create a validation set\n",
    "english_validation = english.sample(n=125, random_state=42)\n",
    "non_english_validation = non_english.sample(n=125, random_state=42)\n",
    "\n",
    "# Remove the validation samples from the training set\n",
    "english = english.drop(english_validation.index)\n",
    "non_english = non_english.drop(non_english_validation.index)\n",
    "\n",
    "val_set = pd.concat([english_validation, non_english_validation])\n",
    "train_set = pd.concat([english, non_english])\n",
    "\n",
    "# Print 50 rows of the validation set\n",
    "val_set.head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "100\n",
      "200\n",
      "(40,)\n",
      "(40,)\n",
      "(40,)\n",
      "(40,)\n",
      "Size of training set:  (5448, 40)\n",
      "Size of validation set:  (250, 40)\n",
      "Number of english speakers in training set:  2724\n",
      "Number of non-english speakers in training set:  2724\n",
      "Number of english speakers in validation set:  125\n",
      "Number of non-english speakers in validation set:  125\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mfcc for all audio files in the english class\n",
    "train_set_mfcc = []\n",
    "train_set_labels = []\n",
    "i = 0\n",
    "for file in train_set:\n",
    "    # Check if file_name exists, if it doesn't, skip it\n",
    "    file_name = 'recordings/recordings/' + file + '.mp3'\n",
    "    if not os.path.exists(file_name):\n",
    "        continue\n",
    "    \n",
    "    # Load the audio file\n",
    "    signal, sample_rate = librosa.load(file_name)\n",
    "    mfcc = extract_mfcc_1d(file_name, n_mfcc=40)\n",
    "    train_set_mfcc.append(mfcc)\n",
    "\n",
    "    # Create a pitch shifted version of the mfcc, first find the sampling rate\n",
    "    # pitch_shifted_mfcc = librosa.feature.mfcc(y=pitch_shift(signal, sample_rate, 2), sr=sample_rate, n_mfcc=40)\n",
    "    pitch_shifted_mfcc = extract_mfcc_1d(pitch_shift(signal, sample_rate, 2), n_mfcc=40, load=False, sample_rate=sample_rate)\n",
    "    train_set_mfcc.append(pitch_shifted_mfcc)\n",
    "\n",
    "    # Create a pitch shifted version of the mfcc, first find the sampling rate\n",
    "    #pitch_shifted_mfcc = librosa.feature.mfcc(y=pitch_shift(signal, sample_rate, -2), sr=sample_rate, n_mfcc=40)\n",
    "    pitch_shifted_mfcc = extract_mfcc_1d(pitch_shift(signal, sample_rate, -2), n_mfcc=40, load=False, sample_rate=sample_rate)\n",
    "    train_set_mfcc.append(pitch_shifted_mfcc)\n",
    "\n",
    "    # Create a noisy version of the mfcc\n",
    "    #noisy_mfcc = librosa.feature.mfcc(y=add_background_noise(signal, 0.005), sr=sample_rate, n_mfcc=40)\n",
    "    noisy_mfcc = extract_mfcc_1d(add_background_noise(signal, 0.005), n_mfcc=40, load=False, sample_rate=sample_rate)\n",
    "    train_set_mfcc.append(noisy_mfcc)\n",
    "\n",
    "    # Create a noisy pitch shifted version of the mfcc\n",
    "    # noisy_pitch_shifted_mfcc = librosa.feature.mfcc(y=add_background_noise(pitch_shift(signal, sample_rate, 2), 0.005), sr=sample_rate, n_mfcc=40)\n",
    "    noisy_pitch_shifted_mfcc = extract_mfcc_1d(add_background_noise(pitch_shift(signal, sample_rate, 2), 0.005), n_mfcc=40, load=False, sample_rate=sample_rate)\n",
    "    train_set_mfcc.append(noisy_pitch_shifted_mfcc)\n",
    "\n",
    "    # Create another noisy pitch shifted version of the mfcc\n",
    "    #noisy_pitch_shifted_mfcc = librosa.feature.mfcc(y=add_background_noise(pitch_shift(signal, sample_rate, -2), 0.005), sr=sample_rate, n_mfcc=40)\n",
    "    noisy_pitch_shifted_mfcc = extract_mfcc_1d(add_background_noise(pitch_shift(signal, sample_rate, -2), 0.005), n_mfcc=40, load=False, sample_rate=sample_rate)\n",
    "    train_set_mfcc.append(noisy_pitch_shifted_mfcc)\n",
    "\n",
    "    # If the file name has english in it, it's an english speaker, otherwise it's a non-english speaker\n",
    "    if 'english' in file:\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "        train_set_labels.append(1)\n",
    "    else:\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "        train_set_labels.append(0)\n",
    "\n",
    "    # Add a counter to keep track of progress\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "# Grab the spectros for all audio files in the non-english class\n",
    "val_set_mfcc = []\n",
    "val_set_labels = []\n",
    "i = 0\n",
    "for file in val_set:\n",
    "    # Check if file_name exists, if it doesn't, skip it\n",
    "    file_name = 'recordings/recordings/' + file + '.mp3'\n",
    "    if not os.path.exists(file_name):\n",
    "        continue\n",
    "\n",
    "    mfcc = extract_mfcc_1d(file_name, n_mfcc=40, load=True)\n",
    "\n",
    "    val_set_mfcc.append(mfcc)\n",
    "\n",
    "    # No need to apply data augmentation to the validation set, add correct label\n",
    "    if 'english' in file:\n",
    "        val_set_labels.append(1)\n",
    "    else:\n",
    "        val_set_labels.append(0)\n",
    "\n",
    "    # Add a counter to keep track of progress\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "# # Calculate the minimum length of all mfccs\n",
    "# min_length = min([mfcc.shape[1] for mfcc in train_set_mfcc + val_set_mfcc])\n",
    "\n",
    "# # Truncate each mfcc in the english_spectro and non_english_spectro lists to the minimum length\n",
    "# train_set_mfcc = [truncate_spectrogram(mfcc, min_length) for mfcc in train_set_mfcc]\n",
    "# val_set_mfcc = [truncate_spectrogram(mfcc, min_length) for mfcc in val_set_mfcc]\n",
    "\n",
    "# # Normalize each mfcc in the english_spectro and non_english_spectro lists using min-max normalization\n",
    "# train_set_mfcc = [min_max_normalize_spectrogram(mfcc) for mfcc in train_set_mfcc]\n",
    "# val_set_mfcc = [min_max_normalize_spectrogram(mfcc) for mfcc in val_set_mfcc]\n",
    "\n",
    "# # Add channel dimension as the first dimension to each mfcc\n",
    "# train_set_mfcc = [np.expand_dims(mfcc, axis=0) for mfcc in train_set_mfcc]\n",
    "# val_set_mfcc = [np.expand_dims(mfcc, axis=0) for mfcc in val_set_mfcc]\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "train_set_mfcc = np.array(train_set_mfcc)\n",
    "val_set_mfcc = np.array(val_set_mfcc)\n",
    "\n",
    "# Convert the labels to numpy arrays\n",
    "train_set_labels = np.array(train_set_labels)\n",
    "val_set_labels = np.array(val_set_labels)\n",
    "\n",
    "# Print shapes of first and last mfcc in both lists\n",
    "print(train_set_mfcc[0].shape)\n",
    "print(train_set_mfcc[-1].shape)\n",
    "\n",
    "print(val_set_mfcc[0].shape)\n",
    "print(val_set_mfcc[-1].shape)\n",
    "\n",
    "# Print the shape of the arrays\n",
    "print(\"Size of training set: \", train_set_mfcc.shape)\n",
    "print(\"Size of validation set: \", val_set_mfcc.shape)\n",
    "\n",
    "# Count the number of english and non-english speakers in the training and validation sets\n",
    "print(\"Number of english speakers in training set: \", np.sum(train_set_labels))\n",
    "print(\"Number of non-english speakers in training set: \", len(train_set_labels) - np.sum(train_set_labels))\n",
    "\n",
    "print(\"Number of english speakers in validation set: \", np.sum(val_set_labels))\n",
    "print(\"Number of non-english speakers in validation set: \", len(val_set_labels) - np.sum(val_set_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a label of 1 to the english class, and 0 to the non-english class\n",
    "# english_labels = np.ones(english_mfcc.shape[0])\n",
    "# non_english_labels = np.zeros(non_english_mfcc.shape[0])\n",
    "\n",
    "# # Combine the english and non-english data into one array\n",
    "# X = np.concatenate((english_mfcc, non_english_mfcc))\n",
    "# y = np.concatenate((english_labels, non_english_labels))\n",
    "\n",
    "# # Print the shape of the combined array\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to npz files\n",
    "np.savez('mfcc_data_40_augmented.npz', X_train=train_set_mfcc, y_train=train_set_labels, X_val=val_set_mfcc, y_val=val_set_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
